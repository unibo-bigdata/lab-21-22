{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62457a32-01e7-48ac-9201-9574837bd56a",
   "metadata": {},
   "source": [
    "# 306 Spark - Page Rank\n",
    "\n",
    "The goal of this lab is to implement the Page Rank algorithm using Spark and RDDs.\n",
    "\n",
    "First, load the datasets on S3:\n",
    "\n",
    "- ```datasets/pr-test.txt``` is a dataset the can be used for testing purpose\n",
    "- ```pr-4.txt``` can be downloaded from [here](https://big.csr.unibo.it/downloads/bigdata/pr-4.txt) (2.7GB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcf000c-3540-46dd-a552-e5629bf3b571",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%configure -f\n",
    "{\"executorMemory\":\"6G\", \"numExecutors\":2, \"executorCores\":2, \"conf\": {\"spark.dynamicAllocation.enabled\": \"false\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d711c526-181f-4724-b468-feed2b2cb755",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "//val bucketname = \"unibo-bd2122-egallinucci\"\n",
    "val bucketname = \"eg-myfirstbucket\"\n",
    "\n",
    "//val mat = \"pr-test\"\n",
    "val mat = \"pr-4\"\n",
    "val path_mat = \"s3a://\"+bucketname+\"/datasets/\"+mat+\".txt\"\n",
    "\n",
    "val nPartitions = if(mat==\"pr-test\") { 2 } else { 50 }\n",
    "\n",
    "\"SPARK UI: Enable forwarding of port 20888 and connect to http://localhost:20888/proxy/\" + sc.applicationId + \"/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b120bdc7-4742-4977-97eb-2518be7cebd4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "// Free the cache\n",
    "for ((k,v) <- sc.getPersistentRDDs) {\n",
    "   v.unpersist()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4026f7-f4b7-42b1-b4aa-99cbb8ba7aa0",
   "metadata": {},
   "source": [
    "## 306-1 PageRank Map-Reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d995d5-b417-45db-8fa9-2a8e3b4247ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.storage.StorageLevel._\n",
    "import org.apache.spark.HashPartitioner\n",
    "\n",
    "val p = new HashPartitioner(nPartitions)\n",
    "\n",
    "val rddM = sc.\n",
    "    textFile(path_mat).\n",
    "    map(r => (r.split(\",\")(0).toInt, r.split(\",\")(1).toInt, r.split(\",\")(2).toDouble)). // (i,j,mij))\n",
    "    map({case (i,j,mij) => (j,(i,mij))}).\n",
    "    partitionBy(p).    \n",
    "    persist(MEMORY_AND_DISK_SER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab405409-056f-4a10-a2cf-283cc59ce503",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val n = rddM.map({case (k,v) => k}).distinct().count()\n",
    "val rddV = sc.\n",
    "    parallelize(List.range(0,n).map(j => (j.toInt,(1/n.toDouble).toDouble))).\n",
    "    partitionBy(p) // (j,vj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84f3490-0e6f-469e-b248-499bc7ed397d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "var iterations = 3\n",
    "\n",
    "var rddV1 = rddV\n",
    "\n",
    "while(iterations > 0){\n",
    "    \n",
    "    rddV1 = rddM.\n",
    "        join(rddV1).\n",
    "        map({case(j,((i,mij),vj)) => (i,mij*vj)}).\n",
    "        reduceByKey(p,_+_)\n",
    "    \n",
    "    iterations = iterations - 1\n",
    "}\n",
    "\n",
    "rddV1.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4060bcd1-10c0-47c2-8867-843ce6b4a4f9",
   "metadata": {},
   "source": [
    "## 306-2 PageRank Map-Reduce - Broadcasting V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e3c7c3-03dc-4675-a9f5-c5750c9757d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "var iterations = 3\n",
    "var v = rddV.collectAsMap()\n",
    "\n",
    "while(iterations > 0){\n",
    "    var bV = sc.broadcast(v)\n",
    "    \n",
    "    val rddV1 = rddM.\n",
    "        map({case(j,(i,mij)) => (i, mij * bV.value.get(j).get)}).\n",
    "        reduceByKey(p,_+_)\n",
    "\n",
    "    iterations = iterations - 1\n",
    "    var v1 = rddV1.collectAsMap()\n",
    "    bV.destroy()\n",
    "    v = v1\n",
    "}\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85ad14f-4192-4393-9daa-e8bee6d36d5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "// Free the cache\n",
    "for ((k,v) <- sc.getPersistentRDDs) {\n",
    "   v.unpersist()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb44a97c-f0af-4f0a-ab30-856ef44f5c75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val rddM1 = sc.\n",
    "    textFile(path_mat).\n",
    "    map(r => (r.split(\",\")(0).toInt, r.split(\",\")(1).toInt, r.split(\",\")(2).toDouble)). // (i,j,mij))\n",
    "    map({case (i,j,mij) => (i,(j,mij))}).\n",
    "    partitionBy(p).    \n",
    "    persist(MEMORY_AND_DISK_SER)\n",
    "\n",
    "val n = rddM1.map({case (k,v) => k}).distinct().count()\n",
    "val rddV = sc.\n",
    "    parallelize(List.range(0,n).map(j => (j.toInt,(1/n.toDouble).toDouble))).\n",
    "    partitionBy(p) // (j,vj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f3b7d5-44a4-44cd-ba7b-984e73728162",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "var iterations = 3\n",
    "var v = rddV.collectAsMap()\n",
    "\n",
    "while(iterations > 0){\n",
    "    var bV = sc.broadcast(v)\n",
    "    \n",
    "    val rddV1 = rddM1.\n",
    "        mapValues({case(j,mij) => (mij * bV.value.get(j).get)}).\n",
    "        reduceByKey(p,_+_)\n",
    "\n",
    "    iterations = iterations - 1\n",
    "    var v1 = rddV1.collectAsMap()\n",
    "    bV.destroy()\n",
    "    v = v1\n",
    "}\n",
    "v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a1a7f2-a11c-4f69-9bac-6dcec8af7cb4",
   "metadata": {},
   "source": [
    "## 306-3 Matrix-Matrix multiplication\n",
    "\n",
    "Use the same datasets to implement a matrix-matrix multiplication"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark",
   "language": "",
   "name": "sparkkernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
